name: ai_story_maker

services:
  web:
    build: .
    container_name: story_web
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - POSTGRES_HOST=db
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - story_network

  db:
    image: postgres:16
    container_name: story_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    env_file:
      - .env
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-story_generator}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - story_network

  rabbitmq:
    image: rabbitmq:3-management
    container_name: story_rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - story_network

  redis:
    image: redis:7-alpine
    container_name: story_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - story_network

  celery_worker:
    build: .
    container_name: story_celery_worker
    command: celery -A config worker -l INFO
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - POSTGRES_HOST=db
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "celery", "-A", "config", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - story_network

  ollama:
    image: ollama/ollama
    container_name: story_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 4G
    networks:
      - story_network

  ollama-init:
    image: curlimages/curl:latest
    container_name: story_ollama_init
    env_file:
      - .env
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== Ollama Init Started ==="
        echo "Target: http://ollama:11434"
        echo "Model: $${OLLAMA_MODEL:-llama3.2:3b}"
        echo ""
        echo "Checking Ollama API..."
        for i in 1 2 3 4 5 6 7 8 9 10; do
          if curl -sf "http://ollama:11434/api/tags" > /dev/null 2>&1; then
            echo "Ollama API is ready!"
            break
          fi
          echo "Attempt $$i: waiting for Ollama..."
          sleep 3
        done
        echo ""
        echo "Pulling model: $${OLLAMA_MODEL:-llama3.2:3b}..."
        curl -X POST "http://ollama:11434/api/pull" \
          -H "Content-Type: application/json" \
          -d "{\"name\": \"$${OLLAMA_MODEL:-llama3.2:3b}\"}" \
          --no-buffer
        echo ""
        echo "=== Model pull complete ==="
    restart: "no"
    networks:
      - story_network

volumes:
  postgres_data:
  rabbitmq_data:
  ollama_data:

networks:
  story_network:
    driver: bridge
